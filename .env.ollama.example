# Example configuration for using Constellation with local Ollama
# Copy this to .env to use Ollama as your AI provider

# AI Provider Configuration
AI_PROVIDER=openai
AI_DEFAULT_PROVIDER=openai

# Ollama Configuration (using OpenAI-compatible API)
OPENAI_API_KEY=ollama
OPENAI_MODEL=qwen3:8b
OPENAI_BASE_URL=http://localhost:11434/v1

# Optional: AI defaults
AI_TEMPERATURE=0.3
AI_MAX_TOKENS=1000
AI_TIMEOUT=30000

# Redis/Valkey Configuration
REDIS_URL=redis://localhost:6379

# Server Configuration
PORT=3000
LOG_LEVEL=info

# Registry Configuration
REGISTRY_PATH=./registry/constellation-registry.yaml

# Tracing (optional)
TRACE_ENABLED=false
# TRACE_ENDPOINT=http://localhost:14268/api/traces

# Metrics (optional)
METRICS_PORT=9090
