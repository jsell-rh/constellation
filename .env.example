# Server Configuration
NODE_ENV=development
PORT=3001  # MCP server port
HOST=localhost
LOG_LEVEL=info

# AI Provider Configuration
# Choose default provider: openai, anthropic, or vertex-ai
AI_DEFAULT_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=sk-your-openai-api-key
OPENAI_MODEL=gpt-4  # Optional, defaults to gpt-4
OPENAI_BASE_URL=https://api.openai.com/v1  # Optional

# For OpenAI-compatible servers (Ollama, TGI, vLLM)
# OPENAI_API_KEY=not-needed
# OPENAI_MODEL=llama2:7b
# OPENAI_BASE_URL=http://localhost:11434/v1

# Anthropic Configuration  
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022  # Optional

# Google Vertex AI Configuration
GOOGLE_PROJECT_ID=your-project-id
GOOGLE_LOCATION=us-central1  # Optional, defaults to us-central1
GOOGLE_MODEL=gemini-1.5-pro  # Optional
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json  # Optional

# Global AI Settings
AI_TEMPERATURE=0.7  # Optional, 0-1 scale
AI_MAX_TOKENS=2048  # Optional
AI_TIMEOUT=30000  # Optional, in milliseconds

# Cache Configuration
VALKEY_URL=redis://localhost:6379
CACHE_TTL=3600

# Authentication
AUTH_ENABLED=true
BEARER_TOKEN_SECRET=your-secret-key-change-in-production
KEYCLOAK_REALM=constellation
KEYCLOAK_AUTH_SERVER_URL=http://localhost:8080/auth
KEYCLOAK_CLIENT_ID=constellation-api
KEYCLOAK_CLIENT_SECRET=your-client-secret

# Observability
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
METRICS_PORT=9090
TRACE_ENABLED=true
METRICS_ENABLED=true

# Service Registry
REGISTRY_PATH=./registry/constellation-registry.yaml
REGISTRY_WATCH=true

# Rate Limiting
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX_REQUESTS=100

# Performance
MAX_CONCURRENT_LIBRARIANS=10
REQUEST_TIMEOUT_MS=30000
AI_REQUEST_TIMEOUT_MS=60000